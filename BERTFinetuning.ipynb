{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import pytorch_lightning as pl\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class \n",
    "class SentimentDataset(data.Dataset):\n",
    "    def __init__(self, tokenizer, text, target, max_len=180):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.max_len =  max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.text[idx]\n",
    "        target = self.target[idx]\n",
    "        \n",
    "        # encode the text and target into tensors return the attention masks as well\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTModel PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, config, model, dim=256, num_classes=3):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # create the model config and BERT initialize the pretrained BERT, also layers wise outputs\n",
    "        self.config = config\n",
    "        self.base = model\n",
    "        \n",
    "        # classifier head [not useful]\n",
    "        self.head = torch.nn.Sequential(*[\n",
    "            torch.nn.Dropout(p=self.config.hidden_dropout_prob),\n",
    "            torch.nn.Linear(in_features=self.config.hidden_size, out_features=dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.config.hidden_dropout_prob),\n",
    "            torch.nn.Linear(in_features=dim, out_features=num_classes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        # first output is top layer output, second output is context of input seq and third output will be layerwise tokens \n",
    "        _, pooled, _ = self.base(input_ids, attention_mask)\n",
    "        outputs = self.head(pooled)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the BERTConfig, BERTTokenizer, and BERTModel \n",
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "bert = BertModel.from_pretrained(model_name, config=config)\n",
    "classifier = BertClassifier(config=config, model=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = classifier(bs['input_ids'], bs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model=None, data_file='./data/train.csv', use_cols=['text', 'target'], batch_size=32):\n",
    "        super(LightningBert, self).__init__()\n",
    "        \n",
    "        # initialize the BERT model c\n",
    "        self.model = model\n",
    "        self.data_file = data_file\n",
    "        self.use_cols = use_cols\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def accuracy(self, outputs, targets):\n",
    "        correct = 0\n",
    "        for i in range(outputs.shape[0]):\n",
    "            if outputs[i]==targets[i]:\n",
    "                correct+=1\n",
    "        return correct/outputs.shape[0]\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs =  model(input_ids, attention_mask)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # first 70% data reserved for validation\n",
    "        train = load_dataset(\"csv\", data_files=self.data_file, split='train[30%:]')\n",
    "        text, target = val['text'], val['target']\n",
    "        dataset = SentimentDataset(tokenizer=tokenizer, text=text, target=target)\n",
    "        loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        return loader\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, targets =  batch['input_ids'], batch['attention_mask'], batch['targets']\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        acc = self.accuracy(logits.argmax(dim=1), targets)\n",
    "        wandb.log({\"Loss\": loss, \"Accuracy\": acc})\n",
    "        return {\"loss\": loss, \"accuracy\": acc}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # first 25% data reserved for validation\n",
    "        train = load_dataset(\"csv\", data_files=self.data_file, split='train[:30%]')\n",
    "        text, target = val['text'], val['target']\n",
    "        dataset = SentimentDataset(tokenizer=tokenizer, text=text, target=target)\n",
    "        loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        return loader\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, targets =  batch['input_ids'], batch['attention_mask'], batch['targets']\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        acc = self.accuracy(logits.argmax(dim=1), targets)\n",
    "        wandb.log({\"Val_loss\": loss, \"Val_acc\": acc})\n",
    "        return {\"Val_loss\": loss, \"Val_acc\": acc}\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the BERTConfig, BERTTokenizer, and BERTModel \n",
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "bert = BertModel.from_pretrained(model_name, config=config)\n",
    "classifier = BertClassifier(config=config, model=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callbacks and wandb logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer and ArgParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There you go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
